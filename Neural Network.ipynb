{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 3-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "data = pd.read_pickle('./alldata.txt')\n",
    "data.head()\n",
    "\n",
    "#normalization\n",
    "X = data.drop(['Label'], axis=1)\n",
    "X_data = (X - X.min())/(X.max() - X.min())\n",
    "X_data = X_data.drop(X_data.columns[X_data.isna().any().tolist()], axis=1)\n",
    "# X_data.head()\n",
    "X_data = np.float32(X_data)\n",
    "y_data = 1 - np.float32(data['Label'] == 'BENIGN') #y = 1 represents anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_data = sess.run(tf.one_hot(y_data, depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninput = 70\n",
    "nlayer1 = 256\n",
    "nlayer2 = 256\n",
    "learning_rate = 0.0001\n",
    "num_steps = 30000\n",
    "display_step = 1000\n",
    "batch_size = 256\n",
    "nclasses = 2\n",
    "# nlayer3 = 256\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, ninput], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, nclasses], name='y')\n",
    "W1 = tf.Variable(tf.random_normal([ninput, nlayer1]))\n",
    "W2 = tf.Variable(tf.random_normal([nlayer1, nclasses]))\n",
    "# W3 = tf.Variable(tf.random_normal([nlayer2, nclasses]))\n",
    "b1 = tf.Variable(tf.random_normal([nlayer1]))\n",
    "b2 = tf.Variable(tf.random_normal([nclasses]))\n",
    "# b3 = tf.Variable(tf.random_normal([nclasses]))\n",
    "# b3 = tf.Variable(tf.random_normal())\n",
    "\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "output = tf.matmul(layer1, W2) + b2\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1981520, 70)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 70)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 2\n",
    "X_train[(cnt - 1) * batch_size : cnt * batch_size, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Minibatch Loss: 0.563367\n",
      "Step 2000: Minibatch Loss: 0.195919\n",
      "Step 3000: Minibatch Loss: 0.177380\n",
      "Step 4000: Minibatch Loss: 0.100634\n",
      "Step 5000: Minibatch Loss: 0.080668\n",
      "Step 6000: Minibatch Loss: 0.096754\n",
      "Step 7000: Minibatch Loss: 0.094645\n",
      "Step 8000: Minibatch Loss: 0.070522\n",
      "Step 9000: Minibatch Loss: 0.125488\n",
      "Step 10000: Minibatch Loss: 0.099386\n",
      "Step 11000: Minibatch Loss: 0.100657\n",
      "Step 12000: Minibatch Loss: 0.077425\n",
      "Step 13000: Minibatch Loss: 0.066098\n",
      "Step 14000: Minibatch Loss: 0.060641\n",
      "Step 15000: Minibatch Loss: 0.091194\n",
      "Step 16000: Minibatch Loss: 0.102623\n",
      "Step 17000: Minibatch Loss: 0.145610\n",
      "Step 18000: Minibatch Loss: 0.065760\n",
      "Step 19000: Minibatch Loss: 0.090371\n",
      "Step 20000: Minibatch Loss: 0.075815\n",
      "Step 21000: Minibatch Loss: 0.071890\n",
      "Step 22000: Minibatch Loss: 0.071415\n",
      "Step 23000: Minibatch Loss: 0.070838\n",
      "Step 24000: Minibatch Loss: 0.072046\n",
      "Step 25000: Minibatch Loss: 0.054004\n",
      "Step 26000: Minibatch Loss: 0.091034\n",
      "Step 27000: Minibatch Loss: 0.064846\n",
      "Step 28000: Minibatch Loss: 0.071721\n",
      "Step 29000: Minibatch Loss: 0.073165\n",
      "Step 30000: Minibatch Loss: 0.074151\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #     sess.run(iterator.initializer)\n",
    "    cnt = 0\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    cntMax = X_train.shape[0]/batch_size\n",
    "    X_test, y_test = shuffle(X_test, y_test)\n",
    "    x_index = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for i in range(1, num_steps+1):\n",
    "        cnt = cnt + 1\n",
    "        if cnt > cntMax:\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            cnt = 1\n",
    "        _, l = sess.run([optimizer, loss], \n",
    "                        feed_dict={X: X_train[(cnt - 1) * batch_size : cnt * batch_size, :], \n",
    "                                   y: y_train[(cnt - 1) * batch_size : cnt * batch_size, :]})\n",
    "        # Display logs per step\n",
    "#         ntest = 0\n",
    "        if i % display_step == 0: # or i == 1:\n",
    "#             ntest += 1;\n",
    "#             x_index.append(i)\n",
    "#             train_loss.append(l)\n",
    "#             l_test = sess.run([loss],\n",
    "#                               feed_dict={X: X_test[(ntest - 1) * batch_size : ntest * batch_size, :],\n",
    "#                                   y: y_test[(ntest - 1) * batch_size : ntest * batch_size]})\n",
    "#             test_loss.append(l_test[0])\n",
    "#           \n",
    "            print('Step %i: Minibatch Loss: %f' % (i, l))\n",
    "    \n",
    "    #Calculate the accuracy over whole set\n",
    "    y_train_pred = sess.run(output, feed_dict={X: X_train, \n",
    "                               y: y_train})\n",
    "    y_test_pred = sess.run(output, feed_dict={X: X_test, \n",
    "                                              y: y_test})\n",
    "#     print(train_pred)\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y1, y2): #y1 = y; y2 = y_pred\n",
    "    return ((y1.argmax(axis=1) == 0) & (y2.argmax(axis=1) == 0)).sum()\n",
    "def FN(y1, y2):\n",
    "    return ((y1.argmax(axis=1) == 0) & (y2.argmax(axis=1) == 1)).sum()\n",
    "def FP(y1, y2):\n",
    "    return ((y1.argmax(axis=1) == 1) & (y2.argmax(axis=1) == 0)).sum()\n",
    "def TN(y1, y2):\n",
    "    return ((y1.argmax(axis=1) == 1) & (y2.argmax(axis=1) == 1)).sum()\n",
    "    \n",
    "def confusionMatrix(y1, y2):\n",
    "    return np.array([[TP(y1, y2), FN(y1, y2)], [FP(y1, y2), TN(y1, y2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mat):\n",
    "    return (mat[0][0] + mat[1][1])/mat.sum()\n",
    "def precision(mat):\n",
    "    return mat[0][0]/(mat[0][0] + mat[1][0])\n",
    "def recall(mat):\n",
    "    return mat[0][0]/(mat[0][0] + mat[0][1])\n",
    "\n",
    "def show_result(mat):\n",
    "    print('Accuracy = ' + str(accuracy(mat)) )\n",
    "    print('Precision = ' + str(precision(mat)) )\n",
    "    print('Recall = ' + str(recall(mat)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1569541,   21509],\n",
       "       [  42685,  347785]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix of train set\n",
    "train_CM = confusionMatrix(y_train, y_train_pred) \n",
    "train_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.967603657798054\n",
      "Precision = 0.9735241833340983\n",
      "Recall = 0.986481254517457\n"
     ]
    }
   ],
   "source": [
    "show_result(train_CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[672706,   9341],\n",
       "       [ 18190, 148986]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix of test set\n",
    "test_CM = confusionMatrix(y_test, y_test_pred) \n",
    "test_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9675809534127079\n",
      "Precision = 0.9736718695722656\n",
      "Recall = 0.9863044628889212\n"
     ]
    }
   ],
   "source": [
    "show_result(test_CM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
